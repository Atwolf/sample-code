import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, label_binarize
from sklearn.metrics import (
    f1_score, precision_recall_curve, roc_auc_score, roc_curve,
    confusion_matrix, average_precision_score
)
import matplotlib.pyplot as plt
from pathlib import Path

from feature_engineering import engineer_features

# === CONFIG ===
REPORTS_DIR = './reports'
LABEL_COL = 'label'
CIRCUIT_ID_COL = 'circuit_id'
MIN_OCCURRENCES = 2
TRAIN_RATIO = 0.8


def load_reports(reports_dir: str) -> pd.DataFrame:
    """Load all xlsx files from directory, sorted alphabetically."""
    files = sorted(Path(reports_dir).glob('*.xlsx'))
    print(f"Found {len(files)} reports")
    
    dfs = []
    for i, path in enumerate(files):
        df = pd.read_excel(path)
        df['report_number'] = i + 1
        df['source_file'] = path.name
        dfs.append(df)
    
    return pd.concat(dfs, ignore_index=True)


def filter_data(df: pd.DataFrame) -> pd.DataFrame:
    """Drop NA labels and circuits with < MIN_OCCURRENCES entries."""
    df = df.dropna(subset=[LABEL_COL])
    
    circuit_counts = df[CIRCUIT_ID_COL].value_counts()
    valid_circuits = circuit_counts[circuit_counts >= MIN_OCCURRENCES].index
    df = df[df[CIRCUIT_ID_COL].isin(valid_circuits)]
    
    print(f"After filtering: {len(df)} rows, {df[CIRCUIT_ID_COL].nunique()} circuits")
    return df


def compute_metrics(y_true, y_pred, y_proba, class_names):
    """Compute F1 and AUC metrics."""
    metrics = {
        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),
        'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0),
        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),
        'f1_per_class': dict(zip(class_names, f1_score(y_true, y_pred, average=None, zero_division=0))),
    }
    
    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))
    
    # Only compute AUC for classes present in test set
    present_mask = y_true_bin.sum(axis=0) > 0
    present_indices = np.where(present_mask)[0]
    
    if len(present_indices) > 1:
        y_true_bin_filtered = y_true_bin[:, present_mask]
        y_proba_filtered = y_proba[:, present_mask]
        
        metrics['auc_roc_macro'] = roc_auc_score(
            y_true_bin_filtered, y_proba_filtered, average='macro', multi_class='ovr'
        )
    else:
        metrics['auc_roc_macro'] = np.nan
    
    metrics['auc_roc_per_class'] = {}
    metrics['avg_precision_per_class'] = {}
    
    for i, cls in enumerate(class_names):
        if y_true_bin[:, i].sum() > 0:
            metrics['auc_roc_per_class'][cls] = roc_auc_score(y_true_bin[:, i], y_proba[:, i])
            metrics['avg_precision_per_class'][cls] = average_precision_score(y_true_bin[:, i], y_proba[:, i])
    
    return metrics


def plot_curves(y_true, y_proba, class_names):
    """Plot and save PR and ROC curves."""
    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))
    n_classes = len(class_names)
    cols = min(4, n_classes)
    rows = (n_classes + cols - 1) // cols
    
    for curve_type in ['pr', 'roc']:
        fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))
        axes = np.array(axes).flatten()
        
        for i, cls in enumerate(class_names):
            if y_true_bin[:, i].sum() > 0:
                if curve_type == 'pr':
                    precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_proba[:, i])
                    ap = average_precision_score(y_true_bin[:, i], y_proba[:, i])
                    axes[i].plot(recall, precision, lw=2)
                    axes[i].set_xlabel('Recall')
                    axes[i].set_ylabel('Precision')
                    axes[i].set_title(f'{cls}\nAP={ap:.3f}')
                else:
                    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_proba[:, i])
                    auc = roc_auc_score(y_true_bin[:, i], y_proba[:, i])
                    axes[i].plot(fpr, tpr, lw=2)
                    axes[i].plot([0, 1], [0, 1], 'k--', lw=1)
                    axes[i].set_xlabel('False Positive Rate')
                    axes[i].set_ylabel('True Positive Rate')
                    axes[i].set_title(f'{cls}\nAUC={auc:.3f}')
                axes[i].set_xlim([0, 1])
                axes[i].set_ylim([0, 1.05])
                axes[i].grid(True, alpha=0.3)
            else:
                axes[i].axis('off')
        
        for i in range(n_classes, len(axes)):
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig(f'{"precision_recall" if curve_type == "pr" else "roc"}_curves.png', dpi=150)
        plt.close()


def create_excel_report(metrics, conf_matrix, class_names, test_df, y_pred):
    """Create Excel workbook with metrics and predictions."""
    test_df = test_df.copy()
    test_df['predicted_label'] = y_pred
    test_df['actual_label'] = test_df[LABEL_COL]
    
    with pd.ExcelWriter('evaluation_report.xlsx', engine='openpyxl') as writer:
        # Summary tab
        summary = pd.DataFrame({
            'Metric': ['F1 (Macro)', 'F1 (Micro)', 'F1 (Weighted)', 'AUC-ROC (Macro)'],
            'Value': [metrics['f1_macro'], metrics['f1_micro'], metrics['f1_weighted'], metrics['auc_roc_macro']]
        })
        per_class = pd.DataFrame([
            {'Class': cls, 'F1': metrics['f1_per_class'].get(cls), 
             'AUC-ROC': metrics['auc_roc_per_class'].get(cls),
             'Avg Precision': metrics['avg_precision_per_class'].get(cls)}
            for cls in class_names
        ])
        summary.to_excel(writer, sheet_name='Summary', index=False)
        per_class.to_excel(writer, sheet_name='Summary', index=False, startrow=len(summary) + 2)
        
        # Confusion matrix tab
        pd.DataFrame(
            conf_matrix,
            index=[f'Actual: {c}' for c in class_names],
            columns=[f'Pred: {c}' for c in class_names]
        ).to_excel(writer, sheet_name='Confusion Matrix')
        
        # Top 5 predicted classes
        for cls in pd.Series(y_pred).value_counts().head(5).index:
            class_df = test_df[test_df['predicted_label'] == cls]
            sheet_name = str(cls)[:31].replace('/', '-').replace('*', '').replace('?', '')
            class_df.to_excel(writer, sheet_name=sheet_name, index=False)


def main():
    # Load and filter
    print("Loading reports...")
    df = load_reports(REPORTS_DIR)
    df = filter_data(df)
    
    # Chronological split
    n_files = df['report_number'].max()
    train_cutoff = int(n_files * TRAIN_RATIO)
    
    train_df = df[df['report_number'] <= train_cutoff]
    test_df = df[df['report_number'] > train_cutoff]
    print(f"Train: {len(train_df)} rows, Test: {len(test_df)} rows")
    
    # Feature engineering
    train_df = engineer_features(train_df)
    test_df = engineer_features(test_df)
    
    cond_cols = [c for c in train_df.columns if c.startswith('cond_')]
    X_train = train_df[cond_cols].fillna(0)
    X_test = test_df[cond_cols].fillna(0)
    
    # Encode labels
    le = LabelEncoder()
    y_train = le.fit_transform(train_df[LABEL_COL])
    y_test = le.transform(test_df[LABEL_COL])
    class_names = list(le.classes_)
    print(f"Features: {len(cond_cols)}, Classes: {len(class_names)}")
    
    # Train
    print("Training...")
    model = RandomForestClassifier(
        n_estimators=200, max_depth=20, class_weight='balanced', random_state=42, n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    # Predict
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)
    
    # Metrics
    metrics = compute_metrics(y_test, y_pred, y_proba, class_names)
    print(f"\nF1 Macro: {metrics['f1_macro']:.4f}")
    print(f"F1 Weighted: {metrics['f1_weighted']:.4f}")
    print(f"AUC-ROC Macro: {metrics['auc_roc_macro']:.4f}")
    
    # Outputs
    print("\nGenerating outputs...")
    plot_curves(y_test, y_proba, class_names)
    create_excel_report(
        metrics, 
        confusion_matrix(y_test, y_pred), 
        class_names, 
        test_df, 
        le.inverse_transform(y_pred)
    )
    print("Done: evaluation_report.xlsx, precision_recall_curves.png, roc_curves.png")


if __name__ == '__main__':
    main()
